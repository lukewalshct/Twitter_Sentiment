import sys
import ast
import json
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils
import requests
from operator import add
import csv
import numpy as np
import threading
import time
from requests_oauthlib import OAuth1Session
import requests_oauthlib
import string
from time import gmtime, strftime

#nltk libraries
import nltk
from nltk.classify import NaiveBayesClassifier
from nltk.classify import NaiveBayesClassifier
from nltk.corpus import subjectivity
from nltk.sentiment import SentimentAnalyzer
from nltk.sentiment.util import *
from nltk.corpus import stopwords

#build nltk training model
train_file = sc.textFile("hdfs://localhost:8020/user/cloudera/final/Sentiment Analysis Dataset.csv")
train_header = train_file.take(1)[0]
train_data_raw = train_file.filter(lambda line: line != train_header)

#split apart the rows for each line and create a tuple
def get_row(line):
  row = line.split(',')
  sentiment = row[1]
  tweet = row[3].strip()
  translator = str.maketrans({key: None for key in string.punctuation})
  tweet = tweet.translate(translator)
  tweet = tweet.split(' ')
  tweet_lower = []
  for word in tweet:
    tweet_lower.append(word.lower())
  return (tweet_lower, sentiment)

train_data = train_data_raw.map(lambda line: get_row(line))
  
sentim_analyzer = SentimentAnalyzer()

#get list of stopwords (with _NEG) to use as a filter
stopwords_all = []
for word in stopwords.words('english'):
  stopwords_all.append(word)
  stopwords_all.append(word + '_NEG')

train_data_sample = train_data.take(10000)
all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in train_data_sample])
all_words_neg_nostops = [x for x in all_words_neg if x not in stopwords_all]

unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg_nostops, top_n=200)
sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)

training_set = sentim_analyzer.apply_features(train_data_sample)

trainer = NaiveBayesClassifier.train
classifier = sentim_analyzer.train(trainer, training_set)

#classify test sentences
test_sentence1 = [(['this', 'program', 'is', 'good_NEG'], '')]
test_sentence2 = [(['I', 'hate', 'is', 'fucking', 'awful'], '')]
test_sentence3 = [(['good', 'wonderful', 'amazing', 'awesome'], '')]
test_set = sentim_analyzer.apply_features(test_sentence1)
test_set2 = sentim_analyzer.apply_features(test_sentence2)
test_set3 = sentim_analyzer.apply_features(test_sentence3)

classifier.classify(test_set[0][0])
classifier.classify(test_set2[0][0])
classifier.classify(test_set3[0][0])



#delete the current spark context
sc.stop()
del sc

key = "apaopGZ2zvfnQPUEu4Dm6OhSs"
secret = "sYTenLWQaUxAHlZshizX8ERbjmtvlMvCwUxM9Z1m1prTIrSGNl"
token = "709905344026320896-s4U8M6rCMDz4CqMRMV2CwBJu8KFKfZG"
token_secret = "Jg8WCL0AZFszLXynsDXOSMcHlynKYThGh4UO8nSu1Kokh"

sample_url = 'https://stream.twitter.com/1.1/statuses/sample.json'
filter_url = 'https://stream.twitter.com/1.1/statuses/filter.json?track=trump'
auth = requests_oauthlib.OAuth1(key, secret, token, token_secret)

BATCH_INTERVAL = 15  # How frequently to update (seconds)
BLOCKSIZE = 100  # How many tweets per update

# Set up spark objects and run
sc  = SparkContext('local[1]', 'TwitterSampleStream')
ssc = StreamingContext(sc, BATCH_INTERVAL)

# Setup Stream
rdd = ssc.sparkContext.parallelize([0])
stream = ssc.queueStream([], default=rdd)

def tfunc(t, rdd):
  return rdd.flatMap(lambda x: stream_twitter_data())

def stream_twitter_data():
  response = requests.get(filter_url, auth=auth, stream=True)
  print(filter_url, response)
  count = 0
  for line in response.iter_lines():
    try:
      if count > BLOCKSIZE:
        break
      post = json.loads(line.decode('utf-8'))
      contents = [post['text']]
      count += 1
      yield str(contents)
    except:
      result = False

stream = stream.transform(tfunc)


def filter_posts(line):
  keywords = ['trump']
  for k in keywords:
    if k.lower() in line[0]:
      return True

  
# Analysis
coord_stream = stream.map(lambda line: ast.literal_eval(line)).filter(filter_posts)

#perform the NLTK analyses and get the 'total' sentiment

def print_sentiment(rdd):
  for line in rdd:
    words = line.split(' ')
    for word in words:
      print(word)

def classify_tweet(tweet):
  sentence = [(tweet, '')]
  test_set = sentim_analyzer.apply_features(sentence)
  print(tweet, classifier.classify(test_set[0][0]))
  return(tweet, classifier.classify(test_set[0][0]))

def get_tweet_text(rdd):
  for line in rdd:
    tweet = line.strip()
    translator = str.maketrans({key: None for key in string.punctuation})
    tweet = tweet.translate(translator)
    tweet = tweet.split(' ')
    tweet_lower = []
    for word in tweet:
      tweet_lower.append(word.lower())
    return(classify_tweet(tweet_lower))

results = []

def output_rdd(rdd):
  global results
  pairs = rdd.map(lambda x: (get_tweet_text(x)[1],1))
  counts = pairs.reduceByKey(add)
  output = []
  for count in counts.collect():
    output.append(count)
  result = [time.strftime("%I:%M:%S"), output]
  results.append(result)
  print(result)

# Convert to something usable....
#coord_stream.foreachRDD(lambda t, rdd: rdd.foreach(lambda rdd: print_sentiment(rdd)))
#coord_stream.foreachRDD(lambda t, rdd: rdd.foreach(lambda rdd: get_tweet_text(rdd)))
coord_stream.foreachRDD(lambda t, rdd: output_rdd(rdd))
#coord_stream.foreachRDD(lambda t, rdd: rdd.map(lambda x: get_tweet_text(x)[1], 1).reduceByKey(add))

# Run!
ssc.start()
ssc.awaitTermination()

#save results
csvfile = '/user/cloudera/final/r'+time.strftime("%I%M%S")+'.csv'
results_rdd = sc.parallelize(results)
results_rdd.saveAsTextFile(csvfile)




